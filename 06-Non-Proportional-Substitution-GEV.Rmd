---
title: "06 Non-Proportional Substitution Patterns I"
output: html_notebook
---

# Non-Proportional Substitution Patterns I: Generalized Extreme Value Models {#chapter-6}

> "Perfection is achieved, not when there is nothing more to add, but when there is nothing left to take away."
>
> --- Antoine de Saint-ExupÃ©ry, Airman's Odyssey

>  "Perfection is a stick with which to beat the possible."
>
> --- Rebecca Solnit, Hope in the Dark


> "The maxim, 'Nothing prevails but perfection,' may be spelled PARALYSIS."
>
> --- Winston S. Churchill

## The limits of perfection

The multinomial logit model is the workhorse of discrete choice analysis. As seen in the preceding chapters, it is a model that is intuitive, and moreover, its closed analytical form makes it simple and convenient to estimate. 

When originally developed by Luce [@Luce1959individual; cited in @Train2009discrete], the logit model was based on the axiom of Independence of Irrelevant Alternatives. As discussed in Chapter \@ref(chapter-3), this property of the logit model implies _proportional substitution patterns_. 

To illustrate this, lets revisit the model estimated in Chapter \@ref(chapter-5). This was a model of the choice of heating systems, with five different systems, namely Gas Central (gc), Gas Room (gr), Electric Central (ec), Electric Room (er), and Heat Pump (hp). 

We will begin by loading the packages needed to estimate this model:
```{r}
library(tidyverse)
library(mlogit)
library(kableExtra)
```

And, we also need to load the dataset used in this section (from the `mlogit` package):
```{r}
data("Heating")
```

The dataset is in "wide" form, which means that there is one record per decision making unit (i.e. per household), so we need to change the data to "long" format:
```{r}
H <- mlogit.data(Heating, shape = "wide", choice = "depvar", varying = c(3:12))
```

Now we are ready to estimate a multinomial logit model (we called this Model 3 in Chapter \@ref(chapter-3)):
```{r}
mod3 <- mlogit(depvar ~ ic + oc, H, reflevel = "ec")
summary(mod3)
```

To explore the patterns of substitution according to this model, we will simulate the adoption rates of the systems after removing one system at the time.

To simulate this situation, we begin by creating a model matrix based on Model 3: 
```{r}
X <- model.matrix(mod3)
```

Then, we remove each one of the alternatives at a time:
```{r}
alt <- index(H)$alt
Xmec <- X[alt != "ec",]
Xmer <- X[alt != "er",]
Xmgc <- X[alt != "gc",]
Xmgr <- X[alt != "gr",]
Xmhp <- X[alt != "hp",]
```

We also want to identify the unique choice ids to reduce the number of choice situations for each decision-maker (from five alternatives to four):
```{r}
# Unique identifiers by decision-maker
chid <- index(H)$chid
# Remove the fifth identifier for each decision-maker
chid <- chid[-seq(1, length(chid), 5)]
```

Based on the above, we can calculate the expression $e^{X\beta}$ and the sum, which are needed to compute the logit probabilities:
```{r}
# After removing ec
exp_Xb_mec <- as.numeric(exp(Xmec %*% coef(mod3)))
sum_exp_Xb_mec <- as.numeric(tapply(exp_Xb_mec, sort(chid), sum))
P_mec <- exp_Xb_mec / sum_exp_Xb_mec[sort(chid)]

# After removing er
exp_Xb_mer <- as.numeric(exp(Xmer %*% coef(mod3)))
sum_exp_Xb_mer <- as.numeric(tapply(exp_Xb_mer, sort(chid), sum))
P_mer <- exp_Xb_mer / sum_exp_Xb_mer[sort(chid)]

# After removing gc
exp_Xb_mgc <- as.numeric(exp(Xmgc %*% coef(mod3)))
sum_exp_Xb_mgc <- as.numeric(tapply(exp_Xb_mgc, sort(chid), sum))
P_mgc <- exp_Xb_mgc / sum_exp_Xb_mgc[sort(chid)]

# After removing gr
exp_Xb_mgr <- as.numeric(exp(Xmgr %*% coef(mod3)))
sum_exp_Xb_mgr <- as.numeric(tapply(exp_Xb_mgr, sort(chid), sum))
P_mgr <- exp_Xb_mgr / sum_exp_Xb_mgr[sort(chid)]

# After removing hp
exp_Xb_mhp <- as.numeric(exp(Xmhp %*% coef(mod3)))
sum_exp_Xb_mhp <- as.numeric(tapply(exp_Xb_mhp, sort(chid), sum))
P_mhp <- exp_Xb_mhp / sum_exp_Xb_mhp[sort(chid)]
```

We can convert the vector of logit probabilities to a matrix, so that each row contains the choice probabilities for a household:
```{r}
# After removing ec
P_mec <- data.frame(matrix(P_mec, ncol = 4, byrow = TRUE))
P_mec <- transmute(P_mec, ec = NA, er = P_mec[, 1], gc = P_mec[, 2], gr = P_mec[, 3], hp = P_mec[, 4])

# After removing er
P_mer <- data.frame(matrix(P_mer, ncol = 4, byrow = TRUE))
P_mer <- transmute(P_mer, ec = P_mer[, 1], er = NA, gc = P_mer[, 2], gr = P_mer[, 3], hp = P_mer[, 4])

# After removing gc
P_mgc <- data.frame(matrix(P_mgc, ncol = 4, byrow = TRUE))
P_mgc <- transmute(P_mgc, ec = P_mgc[, 1], er = P_mgc[, 2], gc = NA, gr = P_mgc[, 3], hp = P_mgc[, 4])

# After removing gr
P_mgr <- data.frame(matrix(P_mgr, ncol = 4, byrow = TRUE))
P_mgr <- transmute(P_mgr, ec = P_mgr[, 1], er = P_mgr[, 2], gc = P_mgr[, 3], gr = NA, hp = P_mgr[, 4])

# After removing hp
P_mhp <- data.frame(matrix(P_mhp, ncol = 4, byrow = TRUE))
P_mhp <- transmute(P_mhp, ec = P_mhp[, 1], er = P_mhp[, 2], gc = P_mhp[, 3], gr = P_mhp[, 4], hp = NA)
```

Given the above, we can summarize the choice probabilities in the form of adoption rates. The table below shows the original adoption rates (when no alternative was removed) and for the different situations of interest, after removing each alternative:
```{r}
df <- data.frame(Alternative = c("None", "ec", "er", "gc", "gr", "hp" ),
  rbind(apply(fitted(mod3, outcome = FALSE), 2, mean),
        apply(P_mec, 2, mean),
        apply(P_mer, 2, mean),
        apply(P_mgc, 2, mean),
        apply(P_mgr, 2, mean),
        apply(P_mhp, 2, mean))
)

df %>%
  kable(col.names = c("Alternative Removed",
                      "ec",
                      "er",
                      "gc",
                      "gr",
                      "hp"),
        digits = 2) %>%
  kable_styling()
```

Examine for a moment the patterns of substitution when alternatives are removed.  In general, the probability of choosing any of the remainder alternatives increases by the same percentage (recall the elasticities), which means that the alternatives that had a higher probability of being chosen to begin with will increase more. Consider, for instance, what happens when Gas Central is removed. Would you say that this pattern of substitution is sensible?

At issue is the possibility that there are _some_ hidden correlations between some alternatives. This does not mean that the model is the problem per se, what it means is that the model was not properly specified. The logit model is the ideal modeling alternative, as long as the utilities are reasonably well-specified. Unfortunately for us, there is no way to say, a priori, whether some alternatives are necessarily correlated given a model specification. 

For instance, in the context of the present example, it is plausible to say that central systems (i.e., Electric Central, Gas Central, Heat Pump) are more similar between them than room systems (i.e., Electric Room, Gas Room). If the specification of the model is not complete and some attribute common to central systems is missing, we would expect the patterns of substitution between those systems to be stronger than between central and room systems. On the other hand, we could also say that the source of energy makes some systems more similar between them, for example, Electric Central, Electric Room, and Heat Pump, whereas Gas Central and Gas Room might be more similar between them. If any relevant attributes missing are common to gas and electric systems, then we would again expect the patterns of substitution to be stronger within a class of systems than between.

A way to capture this is by specifying a branching structure whereby alternatives thought to share similar but missing attributes. Three examples of choice structures are shown in Figure \@ref(fig:fig-choice-structures): a multinomial choice structure, such as in the model above; and two plausible nested choice structures. 
```{r fig-choice-structures, fig.cap= "\\label{fig:fig-choice-structures}Three Examples of Choice Structures", echo=FALSE}
knitr::include_graphics(rep("06-Figure-1.jpg"))
```

The intuition behind the nested choice structures is that decision-makers decide first whether they want an electric- or gas-based system first, before deciding which system specifically; or, as an alternative, that decision-makers decide whether they want a room or central system, before deciding on a specific system. It is important to know that if the systems were highly differentiated through the specification of the utility functions, the nesting structure would be unnecessary. However, since, the question of whether the patterns of substitution should be non-proportional is an empirical one, and the objective of this chapter is to introduce a class of models that are useful to capture non-proportional substitution patterns based on the Extreme Value distribution used to derive the multinomial logit model.

## How to use this note

Remember that the source for the document you are reading is an R Notebook. Throughout the notes, you will find examples of code in segments of text called _chunks_. This is an example of a chunk:
```{r}
print("Do NOT believe a word Gwyneth Paltrow says. Repeat. DO NOT BELIEVE HER.")
```

If you are working with the Notebook version of the document, you can run the code by clicking the 'play' icon on the top right corner of the chunk. If you are reading the web-book version of the document, you will often see that the code has already been executed. You can still try it by copying and pasting into your R or RStudio console.

## Learning objectives

In this practice, you will learn about:

1. The Generalized Extreme Value approach to derive models.
2. The nested logit model.
3. Properties of the nested logit model. 
4. Patterns of substitution with the nested logit model.

## Suggested readings

- Ben-Akiva, M. Lerman, [-@Benakiva1985discrete] Discrete Choice Analysis: Theory and Applications to Travel Demand, **Chapter 5, pp. 126-128 and Chapter 10, pp. 285-299**, MIT Press.
- Hensher, D.A., Rose, J.M., Greene, W.H [-@hensher2005applied] Applied Choice Analysis: A Primer, **Chapters 13 and 14**, Cambridge University Press.
- Louviere, J.J., Hensher, D.A., Swait, J.D. [-@Louviere2000stated] Stated Choice Methods: Analysis and Application, **Chapter 6**, Cambridge University Press.
- Ortuzar JD, Willumsen LG [-@Ortuzar2011modelling] Modelling Transport, Fourth Edition, **Chapter 2, pp. 43-44 and **, John Wiley and Sons.
- Train [-@Train2009discrete] Discrete Choice Methods with Simulation, Second Edition, **Chapter 4**, Cambridge University Press.

## Preliminaries

Load the packages used in this section:
```{r}
library(tidyverse)
library(evd)
library(mlogit)
library(kableExtra)
```

## Generalized Extreme Value models: a recipe for deriving discrete choice models

McFadden's Generalized Extreme Value model [or GEV, for short; @McFadden1978modelling] is a general approach to derive discrete choice models. In this sense, it is not a single model, but rather a recipe for deriving discrete choice models based on the Extreme Value distribution. The definition of the model is discussed here, along with a preliminary example [see @Benakiva1985discrete, pp. 126-127]. Then, we will see how the GEV recipe can be used to derive models with nested structures such as shown in Figure \@ref(fig:fig-choice-structures).

For simplicity, we will postulate that:
$$
y_j=e^{V_j} \text{ , for }j=1,2,\cdots,J
$$
where $J$ is the number of alternatives. Notice that:
$$
y_1\ge0, y_2\ge0, \cdots,y_{J}\ge0
$$

We will define a function $G$ as follows:
$$
G(y_1,y_2, \cdots,y_{J})
$$

Function $G$ must satisfy the following conditions:

1. $G$ is non-negative.

2. $G$ is a homogeneous function of degree one. This means that multiplying every $y_j$ by a factor $\alpha$ is identical to multiplying $G$ by $\alpha$:
$$
G(\alpha y_1, \alpha y_1, \cdots, \alpha y_{J_n}) = \alpha G(y_1, y_2, \cdots, y_{J_n})
$$

3. When any one of $y_i$ grows without bound, $G$ does too:
$$
\lim_{y_j \to \infty}G(y_1,y_2,\cdots,y_{J_n})=\infty \text{, for all }y_j
$$

4. The cross-partial derivatives of the function $G$ change signs in a specific way, alternating between non-positive and non-negative for higher order derivatives:
$$
\begin{array}{c}
   G_j = \frac{\partial G}{\partial y_j}\ge0 \text{, for all }j\\
   G_{jl} = \frac{\partial G}{\partial y_j\partial y_l}\le0 \text{, for all }l\ne j\\   
   G_{jlm} = \frac{\partial G}{\partial y_j\partial y_l\partial y_m}\ge0 \text{, for any }m\ne l \ne j\\  
\end{array}   
$$

A function that satisfies these conditions leads to the following choice probability:
$$
P_i = y_i\frac{G_i}{G}
$$

Train [-@Train2009discrete] notes that the conditions above lack any evident behavioral intuition. While this means that any function $G$ that satisfies them is a legitimate discrete choice probability. On the other hand, the lack of evident intuitions means that someone who is trying to develop a model also lacks guidance in terms of how to translate a desired correlation structure into a function $G$, a limitation that research by Daly and Bierlaire [-@Daly2003general] aimed to address.

For the above reason, we will not be concerned with the particular meaning of the conditions, but rather will see how the GEV framework has been employed to derive various discrete choice models.

We begin in this section with the following function:
$$
G = \sum_{j=1}^{J}y_j
$$

This function satisfies the conditions to be a GEV model, since:

1. $G$ is always non-negative as long as $y_j$ is non-negative for every $j$.

2. The function is homogeneous of degree one, since: 
$$
G(\alpha y_1, \alpha y_1, \cdots, \alpha y_{J}) = \sum_{j=1}^{J}(\alpha y_j) = \alpha \sum_{j=1}^{J}y_j = \alpha G(y_1,y_1, \cdots,y_{J})
$$

3. $G$ grows unbounded if any $y_j$ grows unbounded; and

The first derivative of thiss satisty the condition since:
$$
\begin{array}{c}
   \frac{\partial G}{\partial y_j} =1\\
   \frac{\partial G}{\partial y_j\partial y_l}=0\\  
\end{array}   
$$

The first partial derivative is 1, which satisfies the non-negativity condition. All higher order derivatives are zero, which are non-positive and non-negative as required.

Given this function, we can substitute in the choice probability expression:
$$
P_i = y_i\frac{G_i}{G}=\frac{y_i}{\sum_{j=1}^{J}y_j}
$$

And since:
$$
y_j = e^{V_j}
$$

It follows then that:
$$
P_i =\frac{e^{V_i}}{\sum_{j=1}^{J_n}e^{V_j}}
$$
which is simply the multinomial logit model.

As this illustrates, the multinomial logit is a particular case of a GEV model. Next, we will see how a model with nesting structures is derived.

## Nested Logit model

The nested logit model is derived following the GEV modelling approach by assuming that the $J$ alternatives are uniquely allocated to one class the alternatives, called a nest. There are $S$ nests $B_s$ ($s=1,3,\cdots,S$).

Based on the above, we can define the following function:
$$
G=\sum_{s=1}^S \Big(\sum_{j \in B_s}y_j^{1/\lambda_s} \Big)^{\lambda_s}
$$

We can verify that this function satisfies the conditions to be a GEV model:

1. $G$ is always non-negative since $y_j$ is non-negative for every $j$.

2. The function is homogeneous of degree one, since: 
$$
\begin{eqnarray*}
  G(\alpha y_1, \alpha y_2, \cdots, \alpha y_{J})  &=& \\
  & &\sum_{s=1}^S \Big(\sum_{j \in B_s}(\alpha y)_j^{1/\lambda_s} \Big)^{\lambda_s} = \\
  & &\sum_{s=1}^S \Big(\sum_{j \in B_s}\alpha_j^{1/\lambda_s} y_j^{1/\lambda_s} \Big)^{\lambda_s} = \\
  & &\sum_{s=1}^S \Big(\sum_{j \in B_s}\alpha_j^{1/\lambda_s} y_j^{1/\lambda_s} \Big)^{\lambda_s} = \\
  & &\alpha \sum_{s=1}^S \Big(\sum_{j \in B_s} y_j^{1/\lambda_s} \Big)^{\lambda_s} = \\
  \alpha G(y_1, y_2, \cdots, y_{J})  & & 
\end{eqnarray*}
$$

3. $G$ grows unbounded if any $y_j$ grows unbounded _as long as_ $0 \le \lambda_s \le 1$ for all $s$; and

The first derivative of this function for $j$ in nest $B_t$ is:
$$
\begin{eqnarray*}
  \frac{\partial G}{\partial y_i}  &=& \lambda_t \Big(\sum_{j \in B_t}y_j^{1/\lambda_t} \Big)^{\lambda_t - 1} \frac{1}{\lambda_t} y_i^{1/\lambda_t - 1}= \\
  & &y_i^{1/\lambda_t - 1} \Big(\sum_{j \in B_t}y_j^{1/\lambda_t} \Big)^{\lambda_t - 1} 
\end{eqnarray*}
$$
This derivative is always non-negative, since $y_j$ is non-negative for all $j$.

The second partial derivative is:
$$
\frac{\partial G}{\partial y_i \partial y_l}= y_i^{1/\lambda_t - 1}\frac{\partial}{\partial y_l} \Big(\sum_{j \in B_t}y_j^{1/\lambda_t} \Big)^{\lambda_t - 1}
$$

When $y_l$ is not part of nest $B_t$ the second derivative is zero, which satisfies the non-positive condition. 

When $y_l$ is part of nest $B_t$ the second derivative is:
$$
\begin{eqnarray*}
\frac{\partial G}{\partial y_i \partial y_l} &=& y_i^{1/\lambda_t - 1}\frac{\partial}{\partial y_l} \Big(\sum_{j \in B_t}y_j^{1/\lambda_t} \Big)^{\lambda_t - 1}\\
&=&(\lambda_t - 1)y_i^{1/\lambda_t - 1}\Big(\sum_{j \in B_t}y_j^{1/\lambda_t} \Big)^{\lambda_t - 2} \frac{1}{\lambda_t} y_l^{1/\lambda_t-1}\\
&=&\frac{\lambda_t - 1}{\lambda_t}(y_iy_l)^{1/\lambda_t - 1}\Big(\sum_{j \in B_t}y_j^{1/\lambda_t} \Big)^{\lambda_t - 2}
\end{eqnarray*}
$$

Since $0 \le \lambda_s \le 1$, the term $\frac{\lambda_s - 1}{\lambda_s}$ is at most zero, which again satisfies the non-positive condition. Higher order derivatives satisfy the required conditions.

Since $G$ is a valid function for the GEV recipe, we have that:
$$
\begin{eqnarray*}
P_i &=& y_i\frac{y_i^{1/\lambda_t - 1} \Big(\sum_{j \in B_t}y_j^{1/\lambda_t} \Big)^{\lambda_t - 1}}{\sum_{s=1}^S \Big(\sum_{j \in B_s}y_j^{1/\lambda_s} \Big)^{\lambda_s}}\\
&=&\frac{y_i^{\lambda_t/\lambda_t}y_i^{1/\lambda_t - 1} \Big(\sum_{j \in B_t}y_j^{1/\lambda_t} \Big)^{\lambda_t - 1}}{\sum_{s=1}^S \Big(\sum_{j \in B_s}y_j^{1/\lambda_s} \Big)^{\lambda_s}}\\
&=&\frac{y_i^{1/\lambda_t} \Big(\sum_{j \in B_t}y_j^{1/\lambda_t} \Big)^{\lambda_t - 1}}{\sum_{s=1}^S \Big(\sum_{j \in B_s}y_j^{1/\lambda_s} \Big)^{\lambda_s}}\\
\end{eqnarray*}
$$

Replacing $y_j = e^{V_j}$ gives the expression for the choice probability of the nested model:
$$
P_i = \frac{e^{V_i/\lambda_t} \Big(\sum_{j \in B_s}e^{V_j/\lambda_t} \Big)^{\lambda_t - 1}}{\sum_{s=1}^S \Big(\sum_{j \in B_s}e^{V_j/\lambda_s} \Big)^{\lambda_s}}
$$

Superficially, the probability of the nested model resembles the multinomial probability, but the differences are important. One of the examples of nesting in Figure \@ref(fig:fig-choice-structures) can help to unpack the probability of the nested model.

Suppose that there are $S=2$ nests, namely central ("c") and room ("r"). The probability of choosing an alternative in the nest "central", say Electric Central, is:
$$
P_{ec} = \frac{e^{V_{ec}/\lambda_{c}}(e^{V_{ec}/\lambda_{c}}+e^{V_{gc}/\lambda_{c}}+e^{V_{hp}/\lambda_{c}})^{\lambda_c-1}}
{(e^{V_{ec}/\lambda_{c}}+e^{V_{gc}/\lambda_{c}}+e^{V_{hp}/\lambda_{c}})^{\lambda_c}+(e^{V_{er}/\lambda_{r}}+e^{V_{gr}/\lambda_{r}}+e^{V_{hp}/\lambda_{r}})^{\lambda_r}}
$$

The probability of choosing another alternative in the nest "central", say Gas Central, is:
$$
P_{gc} = \frac{e^{V_{gc}/\lambda_{c}}(e^{V_{ec}/\lambda_{c}}+e^{V_{gc}/\lambda_{c}}+e^{V_{hp}/\lambda_{c}})^{\lambda_c-1}}
{(e^{V_{ec}/\lambda_{c}}+e^{V_{gc}/\lambda_{c}}+e^{V_{hp}/\lambda_{c}})^{\lambda_c}+(e^{V_{er}/\lambda_{r}}+e^{V_{gr}/\lambda_{r}}+e^{V_{hp}/\lambda_{r}})^{\lambda_r}}
$$

Notice that the ratio of odds is:
$$
\frac{P_{ec}}{P_{gc}} = \frac{\frac{e^{V_{ec}/\lambda_{c}}(e^{V_{ec}/\lambda_{c}}+e^{V_{gc}/\lambda_{c}}+e^{V_{hp}/\lambda_{c}})^{\lambda_c-1}}
{(e^{V_{ec}/\lambda_{c}}+e^{V_{gc}/\lambda_{c}}+e^{V_{hp}/\lambda_{c}})^{\lambda_c}+(e^{V_{er}/\lambda_{r}}+e^{V_{gr}/\lambda_{r}}+e^{V_{hp}/\lambda_{r}})^{\lambda_r}}}{\frac{e^{V_{gc}/\lambda_{c}}(e^{V_{ec}/\lambda_{c}}+e^{V_{gc}/\lambda_{c}}+e^{V_{hp}/\lambda_{c}})^{\lambda_c-1}}
{(e^{V_{ec}/\lambda_{c}}+e^{V_{gc}/\lambda_{c}}+e^{V_{hp}/\lambda_{c}})^{\lambda_c}+(e^{V_{er}/\lambda_{r}}+e^{V_{gr}/\lambda_{r}}+e^{V_{hp}/\lambda_{r}})^{\lambda_r}}} = \frac{e^{V_{ec}/\lambda_{c}}}{e^{V_{gr}/\lambda_{r}}} = \frac{e^{V_{ec}}}{e^{V_{gc}}}
$$

As you can see, the ratio of odds is independent from other alternatives: this is the same as the multinomial logit model. In fact, substitution patterns _within_ a nest are proportional, as they are in the case of the multinomial logit.  This is not the case when the ratio of odds is for two alternatives in different nests. For example, the probability of choosing an alternative in the nest for room systems, say "er", is:
$$
P_{er} = \frac{e^{V_{er}/\lambda_{r}}(e^{V_{ec}/\lambda_{r}}+e^{V_{gr}/\lambda_{r}}+e^{V_{hp}/\lambda_{r}})^{\lambda_r-1}}
{(e^{V_{ec}/\lambda_{c}}+e^{V_{gc}/\lambda_{c}}+e^{V_{hp}/\lambda_{c}})^{\lambda_c-1}+(e^{V_{er}/\lambda_{r}}+e^{V_{gr}/\lambda_{r}}+e^{V_{hp}/\lambda_{c}})^{\lambda_r}}
$$

The ratio of odds for electric central and electric room is:
$$
\frac{P_{ec}}{P_{er}} = \frac{\frac{e^{V_{ec}/\lambda_{c}}(e^{V_{ec}/\lambda_{c}}+e^{V_{gc}/\lambda_{c}}+e^{V_{hp}/\lambda_{c}})^{\lambda_c-1}}
{(e^{V_{ec}/\lambda_{c}}+e^{V_{gc}/\lambda_{c}}+e^{V_{hp}/\lambda_{c}})^{\lambda_c}+(e^{V_{er}/\lambda_{r}}+e^{V_{gc}/\lambda_{r}}+e^{V_{hp}/\lambda_{c}})^{\lambda_r}}}{\frac{e^{V_{er}/\lambda_{r}}(e^{V_{er}/\lambda_{r}}+e^{V_{gr}/\lambda_{r}}+e^{V_{hp}/\lambda_{r}})^{\lambda_r-1}}
{(e^{V_{ec}/\lambda_{c}}+e^{V_{gc}/\lambda_{c}}+e^{V_{hp}/\lambda_{c}})^{\lambda_c}+(e^{V_{er}/\lambda_{r}}+e^{V_{gr}/\lambda_{r}}+e^{V_{hp}/\lambda_{r}})^{\lambda_r}}} =
\frac{e^{V_{ec}/\lambda_{c}}(e^{V_{ec}/\lambda_{c}}+e^{V_{gc}/\lambda_{c}}+e^{V_{hp}/\lambda_{c}})^{\lambda_c-1}}{e^{V_{er}/\lambda_{r}}(e^{V_{er}/\lambda_{r}}+e^{V_{gr}/\lambda_{r}}+e^{V_{hp}/\lambda_{r}})^{\lambda_r-1}}
$$

The ratio of odds is no longer independent from other alternatives! In fact, the ratio of odds depends on the alternatives in each of the two nests of interest. This property leads to non-proportional substitution patterns.

## Properties of the nested logit model

As seen above, the nested logit model behaves as the multinomial model within a nest (with proportional substitution), but between nests the patterns of substitution are not necessarily proportional.

An intuitive way to think about the nested logit is in terms of _marginal_ and _conditional_ probabilities.

Suppose that we decompose the systematic utility of the alternatives in the following way:
$$
V_j = Z_j + W_s
$$
where $Z_j$ are the attributes specific to the alternatives, whereas $W_s$ are attributes specific to nest $s$. For example, imagine that central heating systems receive a flat subsidy of $100. Since the alternatives within the nest all share this attribute, and the model operates based on the difference in utilities, this subsidy is not useful to discriminate between alternatives in the "central" nest. However, the subsidy has potentially the effect of making alternatives in the "central" nest more attractive than alternatives in the "room" nest.

Based on the above decomposition, we can rewrite the choice probabilities as follows:
$$
\begin{eqnarray*}
P_i &=& \frac{e^{(Z_i + W_t)/\lambda_t} \Big(\sum_{j \in B_t}e^{(Z_j + W_t)/\lambda_t} \Big)^{-1}\Big(\sum_{j \in B_t}e^{(Z_j + W_t)/\lambda_t} \Big)^{\lambda_t}}{\sum_{s=1}^S \Big(\sum_{j \in B_s}e^{{Z_j + W_s}/\lambda_s} \Big)^{\lambda_s}} \\
&=&\frac{e^{(Z_i + W_t)/\lambda_t} }{\sum_{j \in B_s}e^{(Z_j + W_t)/\lambda_t}}\frac{\Big(\sum_{j \in B_t}e^{(Z_j + W_t)/\lambda_t} \Big)^{\lambda_t}}{\sum_{s=1}^S \Big(\sum_{j \in B_s}e^{(Z_j + W_s)/\lambda_s} \Big)^{\lambda_s}}
\end{eqnarray*}
$$

Lets now take the each term of the probability above in turn. The first term we are going to call the _conditional_ probability:
$$
P_{i|t} = \frac{e^{(Z_i + W_t)/\lambda_t} }{\sum_{j \in B_t}e^{(Z_j + W_t)/\lambda_t}}
$$

We can rearrange this term as follows:
$$
\begin{eqnarray*}
  P_{i|t} &=& \frac{e^{(Z_i + W_s)/\lambda_t} }{\sum_{j \in B_t}e^{(Z_j + W_t)/\lambda_t}}\\
  &=&\frac{e^{Z_i/\lambda_t} e^{W_t/\lambda_t}}{\sum_{j \in B_t}e^{Z_j/\lambda_s} e^{W_t/\lambda_t}}\\
  &=&\frac{e^{Z_i/\lambda_t} e^{W_t/\lambda_t}}{ e^{W_t/\lambda_t}\Big(\sum_{j \in B_t}e^{Z_j/\lambda_t} \Big)}\\
  &=&\frac{e^{Z_i/\lambda_t}}{\sum_{j \in B_t}e^{Z_j/\lambda_t}}
\end{eqnarray*}

$$

The expression above is the probability of choosing alternative $i$ _conditional_ on choosing nest $t$.

Now the second term, which we will call the marginal probability of choosing the nest $t$:
$$
\begin{eqnarray*}
  P_t &=&\frac{\Big(\sum_{j \in B_t}e^{(Z_j + W_t)/\lambda_t} \Big)^{\lambda_t}}{\sum_{s=1}^S \Big(\sum_{j \in B_s}e^{(Z_j + W_s)/\lambda_s} \Big)^{\lambda_s}}\\
  &=&\frac{\Big(\sum_{j \in B_t}e^{Z_j/\lambda_t} e^{W_t/\lambda_t} \Big)^{\lambda_t}}{\sum_{s=1}^S \Big(\sum_{j \in B_s}e^{(Z_j + W_s)/\lambda_s} \Big)^{\lambda_s}}\\
  &=&\frac{\Big(e^{W_t/\lambda_t}\sum_{j \in B_t} e^{Z_j/\lambda_t} \Big)^{\lambda_t}}{\sum_{s=1}^S \Big(e^{W_s/\lambda_s}\sum_{j \in B_s}e^{Z_j/\lambda_s} \Big)^{\lambda_s}}\\
  &=&\frac{e^{W_t}\Big(\sum_{j \in B_t} e^{Z_j/\lambda_t} \Big)^{\lambda_t}}{\sum_{s=1}^S e^{W_s}\Big(\sum_{j \in B_s}e^{Z_j/\lambda_s} \Big)^{\lambda_s}}\\
\end{eqnarray*}
$$

For the last step, we make use of the following property of logarithms:
$$
e^xb^c = e^{x+c\ln b}
$$

Therefore:
$$
\begin{eqnarray*}
  P_t &=&\frac{e^{W_t + \lambda_t\ln\Big(\sum_{j \in B_t} e^{Z_j/\lambda_t} \Big)}}{\sum_{s=1}^S e^{W_s+\lambda_s\ln\Big(\sum_{j \in B_s}e^{Z_j/\lambda_s} \Big)}}\\
  &=&\frac{e^{W_t + \lambda_tI_t}}{\sum_{s=1}^S e^{W_s+\lambda_sI_s}}
\end{eqnarray*}  
$$
with:
$$
I_s = \ln\Big(\sum_{j \in B_s}e^{Z_j/\lambda_s} \Big)
$$
$I_s$ is variously called the _logsum_, or _expected maximum utility_ of a nest. 

It can be appreciated that the probability of choosing alternative $i$ in a nested model is the product of two multinomial logit models in the form of the marginal probability of choosing nest $t$ (sometimes called the upper model), and the probability of choosing $i$ _conditional_ on choosing nest $t$:
$$
P_i = P_{i|t}\cdot P_t= \frac{e^{(Z_i + W_t)/\lambda_t} }{\sum_{j \in B_t}e^{(Z_j + W_t)/\lambda_t}}\ \cdot \frac{e^{W_t + \lambda_tI_t}}{\sum_{s=1}^S e^{W_s+\lambda_sI_s}}
$$

The role of parameters $\lambda_s$ is more intuitive in this formulation of the model. As noted above, $\lambda_s$ is bounded between zero and one. We can examine the limiting cases, as follows:

1. When $\lambda_s <0$ for any $s$, increases in the value of $I_s$ would actually decrease the probability of choosing that nest; a result that is inconsistent with utility maximization. 

2. When $\lambda \to 0$, any changes in the value of $I_s$ would not change the probability of selecting that nest.

3. When $\lambda_s=1$ for all $s$, the choice probability becomes:
$$
P_i = \frac{e^{V_j} \Big(\sum_{j \in B_s}e^{V_j} \Big)^{0}}{\sum_{s=1}^S \Big(\sum_{j \in B_s}e^{V_j} \Big)^{1}}=
\frac{e^{V_j}}{\sum_{k=1}^J e^{V_k}}
$$

The model collapses the the multinomial logit model, since the denominator simply becomes the sum of $e^{V_j}$ for all alternatives across every nest! In fact, $1-\lambda_s$ is a measure of the correlation in the nest, so when $\lambda=1$ this indicates a correlation of zero.

It is possible to test the hypothesis that a nesting parameter is identical to one, by means of a t-test, as follows:
$$
t=\frac{\lambda_s-1}{\sigma_{\lambda_s}}
$$
where $\sigma_{\lambda_s}$ is the standard deviation of the inclusive value.

## Estimation of the nested logit model

The nested logit model can be estimated using `mlogit` by using additional argument in the `mlogit` function. We will illustrate this using the same example. The chunk of code below defines a formula with installation costs (ic) and operation costs (oc) as alternative specific variables with a generic coefficient. In addition, the nests are defined by means of a list. In this model, we define two nests that correspond to "room" systems (er and gr) and "central" systems (ec, gc, and hp). Call this model `nl1` for "nested logit 1":  
```{r}
nl1 <- mlogit(depvar ~ ic + oc, H,
             nests = list(room = c( 'er', 'gr'), central = c('ec', 'gc', 'hp')),
             steptol = 1e-12)
summary(nl1)
```

It can be seen that the coefficients for installation costs and operation costs are significant and have the expected signs. In addition, inclusive values are estimated for each nest. Both coefficients appear as significant; however, in this case, the z-value is calculated in reference to zero. As explained in the preceding section, however, in reality we should think of those coefficients as correlations ($1-\lambda_s$). Accordingly, the correlations for the two nests are:
```{r}
1 - nl1$coefficients["iv:room"]
1 - nl1$coefficients["iv:central"]
```

Clearly, the correlations within the nests are very high. We can test whether the correlation is significant by means of the t-test discussed before:
```{r}
(nl1$coefficients["iv:room"] - 1) / sqrt(vcov(nl1)["iv:room","iv:room"])
(nl1$coefficients["iv:central"] - 1) / sqrt(vcov(nl1)["iv:central","iv:central"])
```

The cutoff value of the t-test at the 5% level of confidence is 1.96. The high values of the t-test indicate that the null hypothesis, i.e., $\lambda_s=1$ can be rejected.

In addition, since the multinomial logit model is a special case of the nested logit model (when $\lambda_s=1$ for all $s$), it is also possible to conduct a likelihood ratio test, as follows:
```{r}
lrtest(mod3, nl1)
```

The likelihood ratio test also indicates that the null hypothesis, i.e., that the nested logit collapses to the multinomial logit, can be rejected at a high level of confidence ($p-val < 0.01$).

It is possible to impose some restrictions to the estimation of the parameters for the nests. The `mlogit` function allows an additional argument that forces all parameters $lambda_s$ to take identical values. The argument is `un.nest.el` (for "unique nest elasticity"), and when set to `TRUE` only one parameter will be estimated (call this Nested Logit Model 2, `nl2`):
```{r}
nl2 <- mlogit(depvar ~ ic + oc, H,
             nests = list(room = c( 'er', 'gr'), central = c('ec', 'gc', 'hp')),
             un.nest.el = TRUE,
             steptol = 1e-12)
summary(nl2)
```

Note how the log-likelihood of the Nested Logit Model 2 is almost identical to the log-likelihood of the Nested Logit Model 1. Since `nl2` is more parsimonious, it makes sense to prefer this model.

## Substitution patterns with the nested logit model

In this section we will revisit the substitution patterns, but now using the nested logit model. To simulate the substitution patterns we will remove each alternative in turn, to examine how the adoption rates change in response.

We begin by copying the model matrix:
```{r}
X <- model.matrix(nl2)
```

Using the model matrix, we can calculate the utilities of each alternative. Notice that each utility is divided by the coefficient of the inclusive value:
```{r}
# Electric central
exp_V_ec <- exp((X[alt == c("ec"), "oc"] * coef(nl2)["oc"] + X[alt == c("ec"), "ic"] * coef(nl2)["ic"]) / coef(nl2)["iv"])

# Gas central
exp_V_gc <- exp((coef(nl2)["gc:(intercept)"] + X[alt == c("gc"), "oc"] * coef(nl2)["oc"] + X[alt == c("gc"), "ic"] * coef(nl2)["ic"]) / coef(nl2)["iv"])

# Heat pump
exp_V_hp <- exp((coef(nl2)["hp:(intercept)"] + X[alt == c("hp"), "oc"] * coef(nl2)["oc"] + X[alt == c("hp"), "ic"] * coef(nl2)["ic"]) / coef(nl2)["iv"])

# Electric room
exp_V_er <- exp((coef(nl2)["er:(intercept)"] + X[alt == c("er"), "oc"] * coef(nl2)["oc"] + X[alt == c("er"), "ic"] * coef(nl2)["ic"]) / coef(nl2)["iv"])

# Gas room
exp_V_gr <- exp((coef(nl2)["gr:(intercept)"] + X[alt == c("gr"), "oc"] * coef(nl2)["oc"] + X[alt == c("gr"), "ic"] * coef(nl2)["ic"]) / coef(nl2)["iv"])
```

The conditional probabilities are the logit models within each nest:
```{r}
#Central, after removing ec
cp_mec_c <- data.frame(gc = exp_V_gc / (exp_V_gc + exp_V_hp),
                       hp = exp_V_hp / (exp_V_gc + exp_V_hp))
#Central, after removing gc
cp_mgc_c <- data.frame(ec = exp_V_ec / (exp_V_ec + exp_V_hp),
                       hp = exp_V_hp / (exp_V_ec + exp_V_hp))
#Central, after removing hp
cp_mhp_c <- data.frame(ec = exp_V_ec / (exp_V_ec + exp_V_gc),
                       gc = exp_V_gc / (exp_V_ec + exp_V_gc))

#Room, after removing central
cp_mc_r <- data.frame(er = exp_V_er / (exp_V_er + exp_V_gr),
                       gr = exp_V_gr / (exp_V_er + exp_V_gr))


#Room, after removing er
cp_mer_r <- data.frame(gr = exp_V_gr / (exp_V_gr))
#Room, after removing gr
cp_mgr_r <- data.frame(er = exp_V_er / (exp_V_er))

#Central, after removing room
cp_mr_c <- data.frame(ec = exp_V_ec / (exp_V_ec + exp_V_gc + exp_V_hp),
                      gc = exp_V_gc / (exp_V_ec + exp_V_gc + exp_V_hp),
                      hp = exp_V_hp / (exp_V_ec + exp_V_gc + exp_V_hp))
```

The marginal probabilities are the logit probabilities of choosing a nest, given the expected maximum utility of each nest:
```{r}
#After removing ec
mp_mec <- data.frame(central = exp(coef(nl2)["iv"] * log(exp_V_gc + exp_V_hp)) 
                       / (exp(coef(nl2)["iv"] * log(exp_V_gc + exp_V_hp)) + 
                            exp((coef(nl2)["iv"] * log(exp_V_er + exp_V_gr)))),
                       room = exp(coef(nl2)["iv"] * log(exp_V_er + exp_V_gr)) / 
                         (exp(coef(nl2)["iv"] * log(exp_V_gc + exp_V_hp)) + 
                            exp((coef(nl2)["iv"] * log(exp_V_er + exp_V_gr))))
                       )
#After removing gc
mp_mgc <- data.frame(central = exp(coef(nl2)["iv"] * log(exp_V_ec + exp_V_hp)) 
                       / (exp(coef(nl2)["iv"] * log(exp_V_ec + exp_V_hp)) + 
                            exp((coef(nl2)["iv"] * log(exp_V_er + exp_V_gr)))),
                       room = exp(coef(nl2)["iv"] * log(exp_V_er + exp_V_gr)) / 
                         (exp(coef(nl2)["iv"] * log(exp_V_ec + exp_V_hp)) + 
                            exp((coef(nl2)["iv"] * log(exp_V_er + exp_V_gr))))
                       )

#After removing hp
mp_mhp <- data.frame(central = exp(coef(nl2)["iv"] * log(exp_V_ec + exp_V_gc)) 
                       / (exp(coef(nl2)["iv"] * log(exp_V_ec + exp_V_gc)) + 
                            exp((coef(nl2)["iv"] * log(exp_V_er + exp_V_gr)))),
                       room = exp(coef(nl2)["iv"] * log(exp_V_er + exp_V_gr)) / 
                         (exp(coef(nl2)["iv"] * log(exp_V_ec + exp_V_gc)) + 
                            exp((coef(nl2)["iv"] * log(exp_V_er + exp_V_gr))))
                       )

#After removing er
mp_mer <- data.frame(central = exp(coef(nl2)["iv"] * log(exp_V_ec + exp_V_gc + exp_V_hp)) 
                       / (exp(coef(nl2)["iv"] * log(exp_V_ec + exp_V_gc + exp_V_hp)) + 
                            exp((coef(nl2)["iv"] * log(exp_V_gr)))),
                       room = exp(coef(nl2)["iv"] * log(exp_V_gr)) / 
                         (exp(coef(nl2)["iv"] * log(exp_V_ec + exp_V_gc + exp_V_hp)) + 
                            exp((coef(nl2)["iv"] * log(exp_V_gr))))
                       )

#After removing gr
mp_mgr <- data.frame(central = exp(coef(nl2)["iv"] * log(exp_V_ec + exp_V_gc + exp_V_hp)) 
                       / (exp(coef(nl2)["iv"] * log(exp_V_ec + exp_V_gc + exp_V_hp)) + 
                            exp((coef(nl2)["iv"] * log(exp_V_er)))),
                       room = exp(coef(nl2)["iv"] * log(exp_V_er)) / 
                         (exp(coef(nl2)["iv"] * log(exp_V_ec + exp_V_gc + exp_V_hp)) + 
                            exp((coef(nl2)["iv"] * log(exp_V_er))))
                       )
```

Once that the conditional and marginal choice probabilities for each case have been calculated, the choice probabilities are the product of the conditional and marginal probabilities:
```{r}
#After removing ec
nlp_mec <- data.frame(cp_mec_c, cp_mc_r, mp_mec) %>% 
  transmute(p_ec = NA,
            p_gc = gc * central,
            p_hp = hp * central,
            p_er = er * room,
            p_gr = gr * room)

#After removing gc
nlp_mgc <- data.frame(cp_mgc_c, cp_mc_r, mp_mgc) %>% 
  transmute(p_ec = ec * central,
            p_gc = NA,
            p_hp = hp * central,
            p_er = er * room,
            p_gr = gr * room)

#After removing hp
nlp_mhp <- data.frame(cp_mhp_c, cp_mc_r, mp_mhp) %>% 
  transmute(p_ec = ec * central,
            p_gc = gc * central,
            p_hp = NA,
            p_er = er * room,
            p_gr = gr * room)

#After removing er
nlp_mer <- data.frame(cp_mr_c, cp_mer_r, mp_mer) %>% 
  transmute(p_ec = ec * central,
            p_gc = gc * central,
            p_hp = hp * central,
            p_er = NA,
            p_gr = gr * room)

#After removing gr
nlp_mgr <- data.frame(cp_mr_c, cp_mgr_r, mp_mgr) %>% 
  transmute(p_ec = ec * central,
            p_gc = gc * central,
            p_hp = hp * central,
            p_er = er * room,
            p_gr = NA)
```

A quick sanity check can be conducted to ensure that the sum of probabilities for each decision-maker is one:
```{r}
summary(rowSums(nlp_mec, na.rm = TRUE))
summary(rowSums(nlp_mgc, na.rm = TRUE))
summary(rowSums(nlp_mhp, na.rm = TRUE))
summary(rowSums(nlp_mer, na.rm = TRUE))
summary(rowSums(nlp_mgr, na.rm = TRUE))
```

Given the above, we can summarize the choice probabilities in the form of adoption rates. The table below shows the original adoption rates (when no alternative was removed) and for the different situations of interest, after removing each alternative:
```{r}
# Original adoption rates
p_o <- apply(fitted(nl2, outcome = FALSE), 2, mean)

df <- data.frame(Alternative = c("None", "ec", "gc", "hp", "er", "gr" ),
  rbind(c(p_o["ec"], p_o["gc"], p_o["hp"], p_o["er"], p_o["gr"]),
        apply(nlp_mec, 2, mean),
        apply(nlp_mgc, 2, mean),
        apply(nlp_mhp, 2, mean),
        apply(nlp_mer, 2, mean),
        apply(nlp_mgr, 2, mean))
)

df %>%
  kable(col.names = c("Alternative Removed",
                      "ec",
                      "gc",
                      "hp",
                      "er",
                      "gr"),
        digits = 2) %>%
  kable_styling()
```
Notice that the patterns of substitution are no longer proportional between nests. Are these substitution patterns more sensible?

## Elasticities of the nested logit model

Recall that the elasticity is the change in probability that results from a one percent change in some attribute. In the case of the multinomial logit model the direct-point elasticity was given by the following expression:
$$
E^{P_{in}}_{x_{ink}} = \beta_{ik}x_{ink}(1-P_{in})
$$
whereas the cross-point elasticity was:
$$
E^{P_{in}}_{x_{ink}} = -\beta_{jk}x_{jnk}P_{jn}
$$
In the case of the nested logit it is also possible to obtain expressions for the direct- and cross-point elasticity, however, these need to take into account the fact that substitution patterns are not proportional. In particular, the parameter of the inclusive value affects the probability of choosing a node and therefore any alternatives contained there.

Accordingly, the direct-point elasticity in the case of a nested logit model is [see @Louviere2000stated, pp. 148-149]:
$$
E^{P_{in}}_{x_{ink}} = \Bigg[(1-P_{t}) + \Big(\frac{1}{\lambda_t-1}\Big)(1-P_{i|t})\Bigg]\beta_{ik}x_{ink}
$$
where $P_t$ is the marginal probability of choosing nest $t$ and $P_{i|t}$ is the probability of choosing $i$ conditional on choosing nest $t$. Note that when an alternative is not nested, the conditional probability is one, and therefore the elasticity is identical to the elasticity of the multinomial logit model.

The cross-elasticity for alternatives in a partition of the nest is:
$$
E^{P_{in}}_{x_{ink}} = -\Bigg[P_{t} + \Big(\frac{1}{\lambda_t-1}\Big)P_{i|t}\Bigg]\beta_{ik}x_{ink}
$$

## Exercise

The blue bus-red bus paradox is a classical illustration of the limitations of the multinomial logit model. This paradox is stated next.

> The blue bus-red bus paradox

There are two initial models, car and blue buses, with systematic utility functions as follows:
$$
V_{blue} = V_{car}
$$

According to the multinomial logit model, the probability of choosing either mode is 0.5, since:
$$
P_{car} = \frac{e^V_{car}}{e^V_{car} + e^V_{blue}} = \frac{e^V_{car}}{e^V_{car} + e^V_{car}} = \frac{1}{2} 
$$
and:
$$
P_{blue} = 1-P_{car}=\frac{1}{2}
$$

A new alternative is introduced. In fact, the new alternative is just some old blue buses painted red. Since consumers do not care about the color of buses, the utility of this new alternative is:
$$
V_{yellow}=V_{bus} = V_{car}
$$

The new choice probabilities are now:
$$
\begin{array}{c}
  P_{car} = \frac{e^V{car}}{e^V{car} + e^V{blue} + e^V{yellow}} = \frac{1}{3}\\
  P_{blue} = \frac{e^V{car}}{e^V{car} + e^V{blue} + e^V{yellow}} = \frac{1}{3}\\
  P_{yellow} = 1 - P_{car} - P_{blue} = \frac{1}{3}
\end{array}
$$
Proportional substitution patterns imply that the new mode (red bus) draws equally from the alternatives, i.e., car and blue buses. Clearly, this does not make sense. An enterpreneur could paint buses in many different colors and reduce the probability of choosing car to zero as a consequence.

1. Restate the blue bus-red bus situation as a nested model. What are the marginal and conditional probabilities of this model?

2. Use model `nl2` in this chapter and calculate the direct-point elasticity at the mean values of the variables, for an increase in the installation costs of Gas Central systems.

2. Use model `nl2` in this chapter and calculate the cross-point elasticity at the mean values of the variables, for an increase in the installation costs of Gas Central systems.

4. Reestimate the nested logit model in this chapter, but change the nests to types of energy as follows:

- Gas: gas central, gas room.
- Electricity: electric central, electric room, heat pump.

Use a single coefficient for the inclusive variables (i.e., set `un.nest.el = TRUE`).

Are the results reasonable? Discuss.


