---
title: "03 Logit"
output: html_notebook
---

# Logit {#chapter-3}

>  "I believe that we do not know anything for certain, but everything probably."
>
> --- Christiaan Huygens

## Modelling choices

In Chapter \@ref(chapter-2) a conceptual framework was described to model choice-making behavior. This framework is based on the economic notion of _utility_, basically that which decision-makers wish to maximize when making choices. The concept of utility has many flaws - key among them that it is not directly observable. If utility could be measured directly by an external observer (or analyst), behavior would seem deterministic. However, unlike Laplace's Demon, an external observer with only human capabilities has limited knowledge of the conditions under which choices are made, if for no other reason that she cannot possibly know the frame of mind of the decision-maker at the moment when choices are made.

A way to implement the conceptual framework unders such conditions involved an acknowledgement that although the decision-maker tries to maximize his utility, some part of it will look random to the observer - therefore the term _random utility modelling_. This allows the analyst to make probabilitistic statements about the behavior of decision-makers. Accordingly, the analyst does not know with certainty the outcome of a choice process, but can quantify her uncertainty in a fairly precise way.

Based on these concepts, Chapter \@ref(chapter-2) concluded by deriving a simple model for discrete choices, namely the lineary probability model [see @Benakiva1985discrete, pp. 66-68]. This model is useful for illustrative purposes. However, it suffers from an important limitation: the linear probabilities are a stepwise function, which makes their mathematical treatment unfun, and also imply that certain outcome are certain (i.e., it can return probabilities of exactly one or exactly zero). This would preclude certain behaviors, which seems a somewhat arrogant thing to do on the part of the analyst. A better approach would be to allow any behavior, but assign very small probabilities to more extreme choices.

In this chapter we will revisit those random utility terms to derive an alternative to the linear probability model. This will be the _logit_ model, one of the most popular models in discrete choice analysis for reasons that will be discussed below.

## How to use this note

Remember that the source for the document you are reading is an R Notebook. Throughout the notes, you will find examples of code in segments of text called _chunks_. This is an example of a chunk:
```{r}
print("Hello, Dr. Train")
```

If you are working with the Notebook version of the document, you can run the code by clicking the 'play' icon on the top right corner of the chunk. If you are reading the web-book version of the document, you will often see that the code has already been executed. You can still try it by copying and pasting into your R or RStudio console.

## Learning objectives

In this practice, you will learn about:

1. The Extreme Value distribution.
2. The binary logit model.
3. The multinomial logit model. 
4. Properties of the logit model.

## Suggested readings

- Ben-Akiva, M. Lerman, [-@Benakiva1985discrete] Discrete Choice Analysis: Theory and Applications to Travel Demand, **Chapters 4 and 5**, MIT Press.
- Hensher, D.A., Rose, J.M., Greene, W.H [-@hensher2005applied] Applied Choice Analysis: A Primer, **Chapter 10**, Cambridge University Press.
- Ortuzar JD, Willumsen LG [-@Ortuzar2011modelling] Modelling Transport, Fourth Edition, **Chapter 7**, John Wiley and Sons.
- Train [-@Train2009discrete] Discrete Choice Methods with Simulation, Second Edition, **Chapter 3**, Cambridge University Press.

## Preliminaries

Load the packages used in this section:
```{r}
library(tidyverse)
library(evd)
```

## Once again those random terms

Recall that in order to implement the probabilitistic statement at the heart of a discrete choice model requires the analyst to make assumptions about the random utility terms. Previously, a number of probability distributions were explored, and one in particular (the uniform distribution) was used to derive a simple discrete choice model. But, is the uniform distribution an appropriate choice for the purpose of modeling the random utility?

The uniform distribution (and some of the other stepwise distributions seen in Chapter \@ref(chapter-2)) are useful to illustrate the concept of probability, and more specifically the need to calculate the area under the curve of the distribution. The area under the curve of the uniform distribution is simply the area of a rectangle, which makes this extremely simple. On the other hand, it precludes certain outcomes, which limits its practical usefulness.

The reality is that, since the utility is in principle unobservable, there is little theoretical support for any specific distribution of the random utility terms. For this reason, the choice of distribution tends to be very pragmatic, particularly attending the convenience for estimation purposes, i.e., retrieving parameters from a sample of observations. The parameters include the parameters used in the systematic utility function $U_{ij}$ (more on this later), as well as any parameters needed for the distribution itself. For instance, the uniform distribution is defined by two parameters, $a$ and $b$:
$$
f(x) = \left\{
        \begin{array}{lc}
            0 & \quad x \le b \\
            \frac{1}{a - b} & \quad b> x > a \\
            0 & \quad x \ge a \\
        \end{array}
    \right.
$$

These parameters define the _dispersion_ of the distribution. The dispersion controls the shape of the distribution, in this case how wide or narrow it is. The greater the difference between $a$ and $b$ the greater the range of values with non-zero probability, but the lower the probability for a constant interval of values. These two parameters also determine the _center_ of the distribution. In this way, the uniform distribution is centered at $\frac{a-b}{2}$. Other distributions also have parameters that determine their shape and position.

A convenient choice of distribution is the Extreme Value type I (EV Type I) probability distribution function. This  function is defined as:
$$
f(x; \mu,\sigma) = e^{-(x + e^{-(x-\mu)/\sigma})}
$$

The EV Type I distribution has two parameters, namely $\mu$ and $\sigma$, which determine the location (i.e., the center) and the dispersion of the distribution, respectively.

The shape of this distribution is shown in Figure \@ref(fig:fig-evi-distribution) with $\mu = 0$ and $\sigma = 1$:

```{r fig-evi-distribution, fig.cap= "\\label{fig:fig-evi-distribution}Extreme Value Type I distribution"}
# Define parameters for the distribution
mu <- 0
sigma <- 1

# Create a data frame for plotting
df <- data.frame(x =seq(from = -5, to = 5, by = 0.01)) %>% mutate(y = dgumbel(x, loc = mu, scale = sigma))

# Plot
ggplot(data = df, aes(x, y)) +
  geom_area(fill = "orange", alpha = 0.5) +
  #ylim(c(0, 1/(2 * L) + 0.2 * 1/(2 * L))) + # Set the limits of the y axis
  geom_hline(yintercept = 0) + # Add y axis
  geom_vline(xintercept = 0) + # Add x axis
  ylab("f(x)") # Label the y axis
```

If you are working with the R Notebook, you can try changing the parameters to see how the function behaves (remember to adjust the limits if you change the center of the distribution!).

The EV Type I has a very interesting property: the difference of two EV Type I distributions follows the logistic distribution. In other words, if $X \sim \text{EVI}(\alpha_Y,\sigma)$ and $Z \sim \text{EVI}(\alpha_Z,\sigma)$, then:
$$
X - Y \sim \text{Logistic}(\alpha_X - \alpha_Y,\sigma)
$$

If we define the difference of the random utility terms as $\epsilon_n = \epsilon_j - \epsilon_k$, the logistic distribution, in turn, is defined as follows:
$$
f(x; \mu,\sigma) = \frac{e^{-(x-\mu)/\sigma}}{\sigma(1 + e^{-(x-\mu)/\sigma})^2}
$$

Whereas the EV Type I distribution was not symmetric, the shape of the logistic distribution is. The logistic distribution is, in fact, similar to the normal distribution but it has fatter tails, which means that the probability of extreme values is higher. This is illustrated in Figure \@ref(fig:fig-logistic-normal-distribution):

```{r fig-logistic-normal-distribution, fig.cap= "\\label{fig:fig-logistic-normal-distribution}Comparison of the logistic (blue) and normal (grey) distributions"}
# Define parameters for the distribution
mu <- 0
sigma <- 1

# Create a data frame for plotting
df <- data.frame(x =seq(from = -5, to = 5, by = 0.01)) %>% 
  mutate(logistic = dlogis(x, location = mu, scale = sigma), normal = dnorm(x, mean = mu, sd = sigma))

# Plot
ggplot() +
  geom_area(data = df, aes(x, logistic), fill = "blue", alpha = 0.5) +
    geom_area(data = df, aes(x, normal), fill = "black", alpha = 0.5) +
  #ylim(c(0, 1/(2 * L) + 0.2 * 1/(2 * L))) + # Set the limits of the y axis
  geom_hline(yintercept = 0) + # Add y axis
  geom_vline(xintercept = 0) + # Add x axis
  ylab("f(x)") # Label the y axis
```

Since the probability expression is given in terms of the difference of the random utilities, if we assume that the random terms $\epsilon$ follow the EV Type I distribution, their difference (i.e., $\epsilon_n = \epsilon_j - \epsilon_i$) follows the logistic distribution. As before, the area under the curve of the function needs to be calculated to obtain a probability. Unfortunately, this needs to be done by integration. Fortunately, this integral has an analytical solution, or so-called _closed form_:
$$
F(x; \mu,\sigma) = \frac{1}{1 + e^{-(\epsilon_n-\mu)/\sigma}}
$$

Accordingly, the probability expression is as follows:
$$
P_j = P(V_j - V_k \le \epsilon_n) = \frac{1}{1 + e^{-(\epsilon_n-\mu)/\sigma}} = \frac{1}{1 + e^{-(V_j-V_k-\mu)/\sigma}}
$$

Which, after some manipulation, can be rewritten as:
$$
P_j = P(V_j - V_k \le \epsilon_n) = \frac{e^{V_j/\sigma}}{e^{V_j/\sigma} + e^{(V_k+\mu)/\sigma}}
$$
The above is called the _logit probability_ and the resulting model is called the _logit model_. As seen, the probability of choosing alternative $j$ is the area under the curve of the logistic distribution function, as seen in Figure \@ref(fig:fig-logistic-distribution) (assuming $mu = 0$ and $\sigma = 1$):

```{r fig-logistic-distribution, fig.cap= "\\label{fig:fig-logistic-distribution}Logit probability"}
# Define parameters for the distribution
mu <- 0
sigma <- 1

# Define an upper limit for calculating the probability
X <- -1

# Create data frames for plotting
df <- data.frame(x =seq(from = -5, to = 5, by = 0.01)) %>% 
  mutate(y = dlogis(x, location = mu, scale = sigma))
df_p <- data.frame(x =seq(from = -5, to = X, by = 0.01)) %>% 
  mutate(y = dlogis(x, location = mu, scale = sigma))

# Plot
ggplot(data = df, aes(x, y)) +
  geom_area(fill = "orange", alpha = 0.5) + # Plot distribution function
  geom_area(data = df_p, fill = "orange", alpha = 1) + # Plot area under the curve
  #ylim(c(0, 1/(2 * L) + 0.2 * 1/(2 * L))) + # Set the limits of the y axis
  geom_hline(yintercept = 0) + # Add y axis
  geom_vline(xintercept = 0) + # Add x axis
  xlab(expression(paste(epsilon[n]))) + # Label the y axis
  ylab("f(x)") # Label the y axis
```

Try changing the upper limit in the figure above to explore the behavior of the logit probability. What is the probability of choosing $j$ when $V_j - V_k = 0$? What is the probability of choosing $j$ when $V_j >> V_k$? And when $V_j << V_k$? Is this as expected?

The cumulative distribution function is shown in Figure \@ref(fig:fig-logistic-cumulative-distribution). Notice that this function tends asymptotically to 0 when $x$ tends to $-\infty$ and to 1 when $x$ tends to $\infty$. This function never assigns values of exactly 0 or exactly 1.

```{r fig-logistic-cumulative-distribution, fig.cap= "\\label{fig:fig-logistic-cumulative-distribution}Linear cumulative distribution function"}
# Create a data frame for plotting
df <- data.frame(x =seq(from = -5, to = 5, by = 0.01)) %>% mutate(y = plogis(x))

# Plot
logit_plot <- ggplot(data = df, aes(x, y)) +
  geom_line(color = "orange") +  # Plot cumulative distribution function
  ylim(c(0, 1)) + # Set the limits of the y axis
  geom_hline(yintercept = 0) + # Add y axis
  geom_vline(xintercept = 0) # Add x axis
logit_plot +
  xlab(expression(paste(V[j], " - ", V[k], sep=""))) +  # Label the x axis
  ylab(expression(paste(P[j]))) # Label the y axis
```

The logit probability exhibits a shape usually called a [_sigmoid_](https://en.wikipedia.org/wiki/Sigmoid_function) (for its resemblance to the letter "s"). This shape is shared by most other discrete choice models - the uniform distribution in Chapter \@ref(chapter-2), for instance, resembled an angular letter "s", whereas the linear and quadratic distribution functions started to display the non-linear aspect of the logit probability function. Sigmoid functions are of interest in many fields. The study of technology adoption is a case in point; new technologies are initially adopted slowly, then go through a rapid growth stage, before reaching saturation. Population growth is often represented by similar curves, with population growing slowly, then explosively, before reaching a carrying capacity limit.

In the case of discrete choice analysis, the shape of the function is interesting from a policy perspective. In the vast majority of cities in North America, for example, the two main modes of transportation are cars and transit. However, the shares of transit tend to be very low, sometimes lower than 10% or even 5%. This suggests that the underlying probabilities of choosing transit at the individual level are very low too.

Suppose that the logit curve in Figure \@ref(fig:fig-logistic-cumulative-distribution) is for the probability of choosing transit. If the initial probability of choosing transit is low, large increases in the utility of transit result in relatively modest gains in probability (see solid blue line in Figure \@ref(fig:fig-logistic-shape-implication)). If the starting probability of transit had been instead 0.5, an identical increase in the utility of transit would result in a much larger gain in the probability (see dashed red line in Figure \@ref(fig:fig-logistic-shape-implication)).

```{r fig-logistic-shape-implication, fig.cap= "\\label{fig:fig-logistic-shape-implication}Implication of the sigmoid shape"}
logit_plot +
  xlab(expression(paste(V[transit], " - ", V[car], sep=""))) + # Label the x axis
  ylab(expression(paste(P[j]))) + # Label the y axis
  annotate("segment", x = -3.75, xend = -2.5, y = 0.024, yend = 0.024, colour = "blue", linetype = "solid") +
  annotate("segment", x = -2.5, xend = -2.5, y = 0.024, yend = 0.075, colour = "blue", linetype = "solid") +
  annotate("segment", x = 0, xend = 1.25, y = 0.5, yend = 0.5, colour = "red", linetype = "dashed") +
  annotate("segment", x = 1.25, xend = 1.25, y = 0.5, yend = 0.77, colour = "red", linetype = "dashed")
```

The implication is that when the penetration of an alternative (think transit, hybrid vehicles, clean energy, and other new technologies) is still low, the incentives needed to raise the probabilities need to be very strong even for modest gains. When penetration has increased, the incentives may be eased since their impact is now more than proportional, until reaching saturation, where again large gains in utility result in modest increases in the probability of adoption. 

## Now, about those parameters $\mu$ and $\sigma$...

Figure \@ref(fig:fig-logistic-distribution) above was created assuming that $\mu=0$ and $\sigma=1$. Can we really set these values in such an arbitrary fashion? The answer is no and yes.

In the case of the centering parameter $\mu$, setting it arbitrarily to zero is not appropriate. The reason is that we can think of this parameter as being key to calculating the difference between the systematic utilities. As seen above, the logit probability is:
$$
P_j = P(V_j - V_k \le \epsilon_n) = \frac{1}{1 + e^{-(V_j-V_k-\mu)/\sigma}}
$$

Assume that we let one of the utility functions absorb $\mu$, that is we let either:
$$
V^*_k = V_k + \mu
$$

or:
$$
V^*_j = V_j - \mu
$$

It does not really matter which utility function we choose to absorb $\mu$ (the only thing that changes is the sign). For convenience, we will say that it is $V_k$, in which case the logit probability can be written as:
$$
P_j = P(V_j - V^*_k \le \epsilon_n) = \frac{1}{1 + e^{-(V_j-V^*_k)/\sigma}}
$$

The difference in other words depends on the value of $\mu$. When $\mu$ is a large positive number, the effect is to increase the utility of alternative $k$ (or conversely, since it would enter with a negative sign in $V^*_j$, it would decrease the utility of alternative $j$). When $\mu$ is a large negative number, the effect is to _increase_ the utility of $j$ - or alternatively to reduce the utility of $k$. For this reason we do not want to arbitrarily set the value of $\mu$ to zero, because this parameter contains information about the relative differences between $V_j$ and $V_k$. The utility function that does not contain the centering parameter $\mu$ is called the _reference_ function.

For simplicity of presentation, I will drop the notation $V^*$ and will assume henceforth that one of the utility functions has absorbed parameter $\mu$.

Now, with respect to the dispersion parameter $\sigma$, this parameter is common to the two utility functions in the logit probability and, as it turns out, it _can_ be arbitrarily set to one. Consider two utility functions as follows:
$$
V_j - V_k
$$

Multiplying (alternatively dividing) by a constant greater than zero changes the _magnitude_ of their difference, since:
$$
\theta(V_j - V_k) = \theta V_j - \theta V_k 
$$

In other words, mutliplying two quantities by a positive constant changes the cardinality of the difference. If you are working with the R Notebook, you might want to try changing the value of `theta` below, keeping in mind that the value must be **greater than zero**:
```{r}
V_j <- -4
V_k <- 8
theta <- 0.8

theta * V_j - theta * V_k 
```

You will notice that the difference changes as you change the value of `theta`. But what about the sign?

On the other hand, multiplying two quantities by a positive constant does not affect their _ordinality_. That is, if $V_j > V_k$ then it is always true that $\theta V_j > \theta V_k$. Recall the decision making rule: an alternative is chosen if its utility is greater than that of the competing alternatives. The rule is purely ordinal, it does not matter if the difference between them is small or large - in other words, their cardinality is irrelevant. This is convenient because it allows us to simplify the logit probability as follows, by arbitrarily setting $\sigma=1$:
$$
P_j = P(V_j - V_k \le \epsilon_n) = \frac{1}{1 + e^{-(V_j-V_k)}} = \frac{e^{V_j}}{e^{V_j} + e^{V_k}}
$$

## Multinomial logit

The logit model above was derived assuming a choice set with only two alternatives. This, of course, is very restrictive, and there are many situations where more than two alternatives are of interest. Fortunately, a multinomial version of the logit model can be derived without much difficulty, and it also results in a closed form expression, as follows:
$$
P_j = P(V_j - V_k \le \epsilon_n) = \frac{e^{V_j}}{\sum_k^Je^{V_k}}
$$

Notice that in this case there are $J-1$ parameters $\mu$ that are absorbed by all but one of the utility functions. As before, it does not matter which utility is selected to act as the reference, since the signs (and magnitudes) of the centering parameters adjust accordingly. More on this later.

## Properties of the logit model

The logit model is the workhorse of discrete choice analysis, in good measure because of its closed form which does not require numerical evaluation of the integrals involved in calculating probabilities (i.e., the "area under the curve", although in multinomial situations this actually is a volume under the surface!)

One important property of the logit model is the way it handles substitution patterns. Consider the ratio of odds for any two alternatives according to the multinomial logit model:
$$
\frac{P_j}{P_m}=\frac{\frac{e^{V_j}}{\sum_ke^{V_k}}}{\frac{e^{V_m}}{\sum_ke^{V_k}}} =\frac{e^{V_j}}{e^{V_m}} =e^{V_j - V_m}
$$

As seen, the ratio of the odds of $P_j$ to $P_m$ depends only on the difference in the utilities of alternatives $j$ and $m$ and nothing more. Furthermore, recall that the choice set is by design an exhaustive set of possible alternatives, and therefore the sum of the probabilities over this set is one:
$$
P_1 + P_2+\cdots+P_J=1
$$

The above means that if the probability of choosing one alternative, say $j$, increases, then the probabilities of choosing some or all of the other alternatives must decline. But since the ratio of odds for any two alternatives is independent of other alternatives in the choice set, the way the probabilities change depends on the change on the probability that triggered the adjustments. This property is called, quite fittingly, _independence from irrelevant alternatives_ or IIA.

Suppose, for instance, that a choice set consists of three alternatives products, say margarine ($m$) by Naturally, and salted butter butter ($sb$) and low-sodium butter ($lb$) by Happy Farms. The initial probabilities of choosing these alternatives are as follows:
$$
\left\{
        \begin{array}{ll}
            P^0_{m}=&\frac{1}{3}\\
            P^0_{sb}=&\frac{1}{3}\\
            P^0_{lb}=&\frac{1}{3}\\
        \end{array}
    \right.
$$

Next, suppose that a change in the attribute set of salted butter ($sb$), for instance a reduction in price, leads to an increase in the probability of choosing this product. Now the probability of choosing salted butter is:
$$
P^1_{sb}=\frac{1}{2}
$$

How do the other probabilities change? On the one hand, we know that the sum of the new probabilities must be one:
$$
P^1_{m} + P^1_{sb} + P^1_{lb} = 1
$$

Since the attributes of margarine and low-sodium butter did not change, we know that their utilities remain unchanged, and therefore:
$$
\frac{P^1_{m}}{P^1_{lb}} = \frac{\frac{1}{3}}{\frac{1}{3}} = 1
$$

In other words, the probability of $P^1_m = P^1_{lb}$. Substituting:
$$
P^1_{m} + P^1_{sb} + P^1_{lb} = 2P^1_{m} + P^1_{sb} = 1
$$

Solving for $P^1_lb$:
$$
P^1_m = \frac{1 - P^1_{sb}}{2} = \frac{1 - \frac{1}{2}}{2} = \frac{1}{4}
$$
Therefore the new probabilities are:
$$
\left\{
        \begin{array}{ll}
            P^1_{m}=&\frac{1}{4}\\
            P^1_{sb}=&\frac{1}{2}\\
            P^1_{lb}=&\frac{1}{4}\\
        \end{array}
    \right.
$$

Notice that the increase in probability of choosing sunflower-based margarine draws proportionally from the other alternatives (i.e., butter and olive oil-based margarine) - in fact, 12.5% from each. Does this result make sense? What is now the market share of Happy Farms-brand line of butter?

The property of Independence from Irrelevant Alternatives leads to proportional substitution patterns. Consider the following initial probabilities:
$$
\left\{
        \begin{array}{ll}
            P^0_{m}=0.5\\
            P^0_{sb}=0.3\\
            P^0_{lb}=0.2\\
        \end{array}
    \right.
$$
The new probability of $sb$ changes to $P^1_{sb}=0.5$. Following the same logic:
$$
\frac{P^1_{m}}{P^1_{lb}} = \frac{0.5}{0.2} = \frac{5}{2} 
$$
And:
$$
P^1_m = \frac{5}{7}(1 - P^1_{sb}) = \Big(\frac{5}{7}\Big)\Big(\frac{1}{2}\Big) = \frac{5}{14} = 0.3571
$$

So the final probabilities are:
$$
\left\{
        \begin{array}{ll}
            P^1_{m}=\frac{5}{14}=0.3571\\
            P^1_{sb}=\frac{1}{2}=0.5000\\
            P^1_{lb}=\frac{2}{14}=0.1429\\
        \end{array}
    \right.
$$

Now, the increase in $P1_{sb}$ to $1/2$ from $P^0_{sb}=1/5$ is drawing _more_ from $P^0_m$ than from $P^0_{lb}$. However, the pattern of substitution is still proportional, as it can be verified:
$$
\begin{array}{ll}
  \frac{P^1_{m}}{P^0_{m}}=\frac{\frac{5}{14}}{\frac{1}{2}}=\frac{10}{14}\\
  \frac{P^1_{lb}}{P^0_{lb}}=\frac{\frac{2}{14}}{\frac{2}{10}}=\frac{10}{14}\\
\end{array}
$$

Proportional substitution patterns are a consequence of the lack of correlation among the random utilities. The logit model considers that the alternatives are all independent. However, in this example, this condition is suspect: the two kinds of margarine are more similar between them than either are to butter. Indeed, if consumers choose butter for flavor, lowering the price of one kind of margarine is likely to draw _less_ than proportionally from the probability of choosing butter - and more than proportionally from the probability of the other kind of margarine if, for instance, consumers prefer margarine for health reasons but respond to price changes.

In this case, the correlation between the two kinds of margarine is a consequence of a missing attribute - say flavor, or health, that is necessary to discriminate among the alternatives. In this way, the logit model can be seen as the ideal model - its closed form being a very attractive feature - as long as the systematic utilities are properly and completely specified. When this is not the case, the results can lead to unrealistic and even unreasonable substition patterns. This issue suggests two possible courses of action:

1. Working to ensure that the systematic utility functions are properly and completely specified.
2. Modifying the modelling apparatus to accommodate correlations among the random utilities.

As will become clear in later chapters, much work in the field of discrete choice analysis has been concerned with the latter. 

## Revisiting the systematic utilities

Much of the discussion above has concentrated on the random utility; however, specifying the systematic utility is key. 

Recall that the utility is a function of the attributes of the alternatives and possibly the attributes of the decision-makers to allow the model to capture heterogeneity in decision-making styles by individuals. the utility function is a convenient way of summarizing all those attributes. Think again of the example of buying a new phone (see Chapter \@ref(chapter-2)). In that simple example, the utilities were a function of three attributes, namely cost, speed, and income - to which we can add the random utility:
$$
\begin{array}{c}
U_{i, \text{Do-Nothing}} = U(\text{cost}_{\text{Do-Nothing}}, \text{ speed}_{\text{Do-Nothing}}, \text{ income}_i) = V(\text{cost}_{\text{Do-Nothing}}, \text{ speed}_{\text{Do-Nothing}}, \text{ income}_i) + \epsilon_{i, \text{Do-Nothing}}\\
U_{i, \text{New-Phone}} = U(\text{cost}_{\text{New-Phone}}, \text{ speed}_{\text{New-Phone}}, \text{ income}_i) = V(\text{cost}_{\text{New-Phone}}, \text{ speed}_{\text{New-Phone}}, \text{ income}_i) + \epsilon_{i, \text{New-Phone}}\\
\end{array} 
$$

A common way of specifying the systematic utility is as linear-in-parameters, something that will be familiar to users of regression analysis:
$$
\begin{array}{c}
V(\text{cost}_{\text{Do-Nothing}}, \text{ speed}_{\text{Do-Nothing}}, \text{ income}_i) = \beta_1\text{cost}_{\text{Do-Nothing}} + \beta_2\text{ speed}_{\text{Do-Nothing}} + \beta_3\text{ income}_i\\
V(\text{cost}_{\text{New-Phone}}, \text{ speed}_{\text{New-Phone}}, \text{ income}_i) = \mu + \beta_1\text{cost}_{\text{New-Phone}} + \beta_2\text{ speed}_{\text{New-Phone}} + \beta_3\text{ income}_i\\
\end{array} 
$$
Notice how the location parameter of the logistic function is absorbed by one of the utility functions!

The additive form of the utilities reflects a compensatory choice-making strategy: higher costs may be offset by higher speeds, for example. An important consideration is the way attributes enter the utility functions. Recall that one way of writing the logit probability was:
$$
P_j = \frac{1}{1 + e^{-(V_j-V_k)}}
$$
This formulation makes it clear that the probability is a function of the differences between utilities (this remains true in the multinomial logit, even if it is not as clear to see). Now consider what happens when the differences in utility are calculated:
$$
V_{i,\text{Do-Nothing}} - V_{i,\text{New-Phone}}= \beta_1\text{cost}_{i,\text{Do-Nothing}} + \beta_2\text{ speed}_{i,\text{Do-Nothing}} + \beta_3\text{income}_i - \mu - \beta_1\text{cost}_{i,\text{New-Phone}} - \beta_1\text{speed}_{\text{i, New-Phone}} - \beta_1\text{income}_i\\ 
= \beta_1(\text{cost}_{i, \text{Do-Nothing}} - \text{cost}_{i, \text{New-Phone}}) + \beta_2(\text{ speed}_{i, \text{Do-Nothing}} - \text{ speed}_{i, \text{New-Phone}}) + \beta_3(\text{ income}_i - \text{ income}_i) - \mu
$$
The income attribute vanishes!

It is useful to distinguish between attributes that vary across utility functions and those that do not. Level of service attributes, those that describe the alternatives, generally vary by utility function - indeed, it is those attributes that help discriminate between alternatives. In this instance, income is invariant across utility functions. Personal attributes of the decision-makers, in general, are invariant across utility functions.

The most common way of dealing with attributes that are constant across utility functions is to select one utility to act as a reference and set that attribute to zero there. This is illustrated below:
$$
\begin{array}{c}
V_{\text{Do-Nothing}} = \beta_1\text{cost}_{\text{Do-Nothing}} + \beta_2\text{ speed}_{\text{Do-Nothing}} + \beta_3(0)\\
V_{\text{New-Phone}} = \mu + \beta_1\text{cost}_{\text{New-Phone}} + \beta_2\text{ speed}_{\text{New-Phone}} + \beta_3\text{ income}_i\\
\end{array} 
$$
The difference in utilities then becomes:
$$
V_{i,\text{Do-Nothing}} - V_{i,\text{New-Phone}}= \beta_1(\text{cost}_{i, \text{Do-Nothing}} - \text{cost}_{i, \text{New-Phone}}) + \beta_2(\text{ speed}_{i, \text{Do-Nothing}} - \text{ speed}_{i, \text{New-Phone}}) - \beta_3(\text{ income}_i) - \mu
$$
When the effect of income is positive (i.e., $\beta_3>0$) higher incomes reduce the probability of doing nothing, and when the effect of income is negative (i.e., $\beta_3<0$) higher incomes reduce the probability of buying a new phone. The effect of income is relative to the reference alternative. When there are more than two alternatives, the attribute can be entered in all but the reference utility, as shown next:
$$
\begin{array}{llll}
V_{\text{Do-Nothing}} = &0 &+ 0 &+ \beta_1\text{cost}_{\text{Do-Nothing}} &+ \beta_2\text{ speed}_{\text{Do-Nothing}} &+ \beta_3(0)& + \beta_4(0) \\
V_{\text{uPhone}} = &\mu_{\text{uPhone}} &+ 0 &+ \beta_1\text{cost}_{\text{uPhone}} &+ \beta_2\text{ speed}_{\text{uPhone}} &+ \beta_3\text{ income}_i & + \beta_4(0)\\
V_{\text{zPhone}} = &0 &+ \mu_{\text{zPhone}} &+ \beta_1\text{cost}_{\text{zPhone}} &+ \beta_2\text{ speed}_{\text{zPhone}} &+ \beta_3(0) &+ \beta_4\text{ income}_i\\
\end{array} 
$$
The above also illustrates how location parameters are absorbed by $J-1$ utility functions.

Another way to introduce attributes that do not vary across utility functions is reminiscent of Casetti's expansion method [@Casetti1972expansion]. The expansion method is a systematic approach to introduce variable interactions that proceeds by defining an initial model whose coefficients are subsequently expanded using contextual variables. Suppose that the initial model is comprised of the utility functions with only level of service variables:
$$
\begin{array}{c}
V_{\text{Do-Nothing}} = \beta_1\text{cost}_{\text{Do-Nothing}} + \beta_2\text{ speed}_{\text{Do-Nothing}}\\
V_{\text{New-Phone}} = \mu + \beta_1\text{cost}_{\text{New-Phone}} + \beta_2\text{ speed}_{\text{New-Phone}}\\
\end{array}
$$

The coefficients are expanded by a contextual variable, in this case income:
$$
\beta_1 = \beta_{11} + \beta_{12}\text{income}_i\\
\beta_2 = \beta_{21} + \beta_{22}\text{income}_i
$$

Substituting the expanded coefficients in the initial model:
$$
\begin{array}{c}
V_{\text{Do-Nothing}} = (\beta_{11} + \beta_{12}\text{income}_i)\text{cost}_{\text{Do-Nothing}} + (\beta_{21} + \beta_{22}\text{income}_i)\text{ speed}_{\text{Do-Nothing}}\\
V_{\text{New-Phone}} = \mu + (\beta_{11} + \beta_{12}\text{income}_i)\text{cost}_{\text{New-Phone}} + (\beta_{21} + \beta_{22}\text{income}_i)\text{ speed}_{\text{New-Phone}}\\
\end{array}
$$

The expanded model then becomes:
$$
\begin{array}{c}
V_{\text{Do-Nothing}} = \beta_{11}\text{cost}_{\text{Do-Nothing}} + \beta_{12}\text{income}_i\cdot\text{cost}_{\text{Do-Nothing}} + \beta_{21}\text{ speed}_{\text{Do-Nothing}} + \beta_{22}\text{income}_i\cdot\text{ speed}_{\text{Do-Nothing}}\\
V_{\text{New-Phone}} = \mu + \beta_{11}\text{cost}_{\text{New-Phone}} + \beta_{12}\text{income}_i\cdot\text{cost}_{\text{New-Phone}} + \beta_{21}\text{ speed}_{\text{New-Phone}} + \beta_{22}\text{income}_i\cdot\text{ speed}_{\text{New-Phone}}\\
\end{array}
$$

The difference of the two utilities in turn is:
$$
\begin{array}{l}
V_{\text{Do-Nothing}} - V_{\text{New-Phone}} =\\
\beta_{11}(\text{cost}_{\text{Do-Nothing}} - \text{cost}_{\text{New-Phone}}) + \beta_{12}\text{income}_i\cdot(\text{cost}_{\text{Do-Nothing}} - \text{cost}_{\text{New-Phone}} ) \\
+ \beta_{21}(\text{ speed}_{\text{Do-Nothing}} - \text{ speed}_{\text{New-Phone}}) + \beta_{22}\text{income}_i\cdot(\text{ speed}_{\text{Do-Nothing}} - \text{ speed}_{\text{New-Phone}}) - \mu
\end{array}
$$

Specifying the utility functions is more art than technique. We will return to this when we begin the practice of model estimation.

## Exercise

Answer the following questions.

### Questions

1. What do we mean when we say that the logit probability has a closed form?

2. Why is it that we can set the dispersion parameter in the logit probabilities to one?

Suppose that a choice set consists of two alternatives, travel by car ($c$) and travel by blue bus ($bb$). The utilities of these two modes are the same, that is:
$$
V_c = V_{bb}
$$

3. What are the probabilities of choosing these two modes? 

Suppose that the transit operator decides to introduce a new service, namely a red bus. This red bus is identical to the blue bus in every respect except the color. 

4. Under these new conditions, what are the logit probabilities of choosing these modes?

5. Discuss the results of introducing a new mode in the choice process above.